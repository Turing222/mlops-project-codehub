"""
import gc

import torch

from backend.core.docling_models import AIModelFactory


class IngestionService:
    def __init__(self, uow):
        self.uow = uow

    async def process_file(self, file_path: str, file_id: int):
        converter = AIModelFactory.get_docling_converter()
        embed_model = AIModelFactory.get_embed_model()

        # 1. è§£æ (CPU/GPU å¯†é›†)
        result = converter.convert(file_path)
        md_content = result.document.export_to_markdown()

        # 2. åˆ†å—å¹¶å‘é‡åŒ–
        chunks = md_content.split("\n\n")  # ç®€å•æ¼”ç¤ºï¼Œå»ºè®®æ ¹æ® Markdown æ ‡é¢˜åˆ†
        to_db = []
        for text in chunks:
            if len(text.strip()) < 10:
                continue
            # å‘é‡åŒ–
            embedding = embed_model.encode(text, normalize_embeddings=True).tolist()
            to_db.append({"file_id": file_id, "content": text, "embedding": embedding})

        # 3. å…¥åº“

        await self.uow.knowledge.add_chunks(to_db)
        await self.uow.commit()

        # 4. ğŸ’¡ 16G å†…å­˜ä¿å‘½æ“ä½œï¼šæ‰‹åŠ¨æ¸…ç†
        del chunks
        del to_db
        torch.cuda.empty_cache()  # æ¸…ç†æ˜¾å­˜ç¢ç‰‡
        gc.collect()  # æ¸…ç†ç³»ç»Ÿå†…å­˜
"""
